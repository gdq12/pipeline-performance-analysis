{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c52c4-7a80-4d0f-8aba-0dc9923a7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed libararies \n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import logging \n",
    "import time\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud import bigquery\n",
    "from dict_helpers import schema_yellow_dict2009, schema_yellow_dict2010, schema_yellow_dict, schema_green_dict, schema_fhv_dict, schema_fhvhv_dict\n",
    "# from dict_helpers import schema_yellow_dict2009_types, schema_yellow_dict2010_types, schema_yellow_dict_types, schema_green_dict_types, schema_fhv_dict_types, schema_fhvhv_dict_types\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from py4j.protocol import Py4JJavaError\n",
    "\n",
    "def parquet_downloaded(data):\n",
    "    if type(data) != str:\n",
    "        return True\n",
    "    else:\n",
    "        return False \n",
    "        \n",
    "def parquet_not_downloaded(data):\n",
    "    if data == None:\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "\n",
    "def abort_pipeline():\n",
    "    print(\"faulty parquet file downloaded, iteration run aborted\")\n",
    "\n",
    "################################################local######################################################################\n",
    "def load_trip_data(spark, url, filename):\n",
    "    \n",
    "    # down load parquet\n",
    "    print(f'fetching {filename} from {url}')\n",
    "    os.system(f'curl -O {url}') \n",
    "    \n",
    "    try:\n",
    "        # read parquet into environment \n",
    "        df = spark.read \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .parquet(filename)\n",
    "        return df\n",
    "    except Py4JJavaError as e:\n",
    "        os.system(f\"rm -r {filename}\")\n",
    "        print(f\"{filename} not a valid parquet file, aborted pipeline!!!!\") \n",
    "        return None\n",
    "\n",
    "#############################################dataproc######################################################################\n",
    "# def load_trip_data(spark, url, filename, bucket_name):\n",
    "    \n",
    "#     # down load parquet\n",
    "#     print(f'fetching {filename} from {url}')\n",
    "#     os.system(f'curl -O {url}') \n",
    "#     os.system(f'gsutil -m cp {filename} gs://original_parquets_url/')\n",
    "    \n",
    "#     try:\n",
    "#         # read parquet into environment \n",
    "#         df = spark.read \\\n",
    "#             .option(\"header\", \"true\") \\\n",
    "#             .parquet(f\"gs://original_parquets_url/{filename}\")\n",
    "#         print(f'loaded gs://original_parquets_url/{filename} in spark session ')\n",
    "#         return df\n",
    "#     except Py4JJavaError as e:\n",
    "#         # os.system(f\"rm -r {filename}\")\n",
    "#         print(f\"aborted pipeline for gs://original_parquets_url/{filename} due to error\")\n",
    "#         return None\n",
    "###########################################################################################################################\n",
    "\n",
    "def dimension_name_cleanup(df):\n",
    "    print(f\"Spark DF currently has the following columns: {', '.join(df.columns)}\")\n",
    "    # cleanup column names \n",
    "    for i in range(len(df.columns)):\n",
    "        col_name = df.columns[i]\n",
    "        col_name_new = re.sub('(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z]{1}[a-z])', '_', col_name).lower()\n",
    "        df = df.withColumnRenamed(col_name, col_name_new) \n",
    "    print(f\"Spark DF has been updated with the following columns: {', '.join(df.columns)}\")\n",
    "    return df\n",
    "\n",
    "###########################################local##########################################################################\n",
    "\n",
    "def data_2_gcp_cloud_storage(df, table_name, year_month, root_path, filename):\n",
    "    # repartition df for export \n",
    "    pickup_col = [col for col in df.columns if 'pickup' in col][0]\n",
    "    col_name = [re.sub('datetime|date_time', 'date', col) for col in df.columns if 'pickup' in col][0]\n",
    "    \n",
    "    df = df.withColumn('parquet_source_path', F.lit(root_path)) \\\n",
    "            .alias('a') \\\n",
    "            .select('a.*', \\\n",
    "                    F.date_trunc('day', f'a.{pickup_col}') \\\n",
    "                    .alias(col_name))\n",
    "    \n",
    "    print(f\"number of unique partitions for DF: {df.select(col_name).distinct().count()}\")\n",
    "\n",
    "    # repartition and save locally \n",
    "    os.system(f'mkdir {table_name}_{year_month}')\n",
    "\n",
    "    print(f\"saving partitions in {table_name}_{year_month}\")\n",
    "    \n",
    "    df.repartition(col_name) \\\n",
    "        .write.parquet(f'{table_name}_{year_month}', mode = 'overwrite') \n",
    "\n",
    "    # print(f'removing unneeded _SUCCESS file from folder')\n",
    "    os.system(f'rm -r {table_name}_{year_month}/_SUCCESS')\n",
    "\n",
    "    print(f\"loading parquets to {root_path}\")\n",
    "    \n",
    "    # copy parquets to cloud storage\n",
    "    os.system(f'gsutil -m cp -r {table_name}_{year_month}/*.parquet {root_path}')\n",
    "    \n",
    "    print('cleaning up environment for next run')\n",
    "    # remove parquets locally to make sure pull from gcp worked\n",
    "    os.system(f'rm -r {table_name}_{year_month}')\n",
    "    os.system(f'rm {filename}')\n",
    "\n",
    "#########################################dataproc##########################################################################\n",
    "#\n",
    "# def data_2_gcp_cloud_storage(df, root_path):\n",
    "#     # repartition df for export \n",
    "#     pickup_col = [col for col in df.columns if 'pickup' in col][0]\n",
    "#     col_name = [re.sub('datetime|date_time', 'date', col) for col in df.columns if 'pickup' in col][0]\n",
    "    \n",
    "#     df = df.alias('a') \\\n",
    "#             .select('a.*', \\\n",
    "#                     F.date_trunc('day', f'a.{pickup_col}') \\\n",
    "#                     .alias(col_name))\n",
    "    \n",
    "#     print(f\"number of unique partitions for DF: {df.select(col_name).distinct().count()}\")\n",
    "\n",
    "#     # repartition and save locally \n",
    "#     os.system(f'gcloud storage folders create {root_path}')\n",
    "\n",
    "#     print(f\"loading parquets to {root_path}\")\n",
    "    \n",
    "#     df.repartition(col_name) \\\n",
    "#         .write.parquet(root_path, mode = 'overwrite')\n",
    "\n",
    "#     print(f'removing unneeded _SUCCESS file from folder')\n",
    "#     os.system(f'gsutil rm -r {root_path}/_SUCCESS')\n",
    "    \n",
    "#     print(f\"loading partitions to {root_path} complete\")\n",
    "###########################################################################################################################\n",
    "\n",
    "def bucket_2_bigquery(gcp_project_name, table_name, root_path):\n",
    "    \n",
    "    # connecting to bigquery \n",
    "    credentials = service_account.Credentials.from_service_account_file(os.getenv('GOOGLE_APPLICATION_CREDENTIALS'))\n",
    "    project_id = gcp_project_name\n",
    "\n",
    "    # cloud storage centric vars \n",
    "    table_name = table_name\n",
    "\n",
    "    # query centric vars \n",
    "    if 'yellow_tripdata_2009' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_yellow_dict2009.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_yellow_dict2009.keys()])[:-1]\n",
    "    elif 'yellow_tripdata_2010' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_yellow_dict2010.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_yellow_dict2010.keys()])[:-1]\n",
    "    elif 'yellow' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_yellow_dict.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_yellow_dict.keys()])[:-1]\n",
    "    elif 'green' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_green_dict.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_green_dict.keys()])[:-1]\n",
    "    elif 'fhv' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_fhv_dict.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_fhv_dict.keys()])[:-1]\n",
    "    elif 'fhvhv' in root_path:\n",
    "        col_param = ' '.join([key + ' ' + item + ',' for key, item in schema_fhvhv_dict.items()])[:-1]\n",
    "        col_names = ' '.join([key + ',' for key in schema_fhvhv_dict.keys()])[:-1]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    q1a = f\"\"\"create schema if not exists `{project_id}`.`nytaxi_raw`\n",
    "    options (location = 'EU')\n",
    "    \"\"\"\n",
    "\n",
    "    q1b = f\"\"\"create schema if not exists `{project_id}`.`nytaxi_stage`\n",
    "    options (location = 'EU')\n",
    "    \"\"\"\n",
    "\n",
    "    q1c = f\"\"\"create schema if not exists `{project_id}`.`nytaxi_transform`\n",
    "    options (location = 'EU')\n",
    "    \"\"\"\n",
    "\n",
    "    q1d = f\"\"\"create schema if not exists `{project_id}`.`nytaxi_prod`\n",
    "    options (location = 'EU')\n",
    "    \"\"\"\n",
    "\n",
    "    q2 = f\"\"\"create or replace external table `{project_id}`.`nytaxi_raw.external_{table_name}`\n",
    "    options (\n",
    "    format = 'PARQUET',\n",
    "    uris = ['{root_path}/*']\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    if 'yellow_tripdata_2009' in root_path:\n",
    "        q3a = f\"\"\"create table if not exists `{project_id}`.`nytaxi_stage`.`{table_name}_2009`\n",
    "        ({col_param})\n",
    "        \"\"\"\n",
    "        q3b = f\"\"\"insert into `{project_id}`.`nytaxi_stage`.`{table_name}_2009`\n",
    "        ({col_names})\n",
    "        select *, current_timestamp() from `{project_id}.nytaxi_raw.external_{table_name}`\n",
    "        \"\"\"\n",
    "    elif 'yellow_tripdata_2010' in root_path:\n",
    "        q3a = f\"\"\"create table if not exists `{project_id}`.`nytaxi_stage`.`{table_name}_2010`\n",
    "        ({col_param})\n",
    "        \"\"\"\n",
    "        q3b = f\"\"\"insert into `{project_id}`.`nytaxi_stage`.`{table_name}_2010`\n",
    "        ({col_names})\n",
    "        select *, current_timestamp() from `{project_id}.nytaxi_raw.external_{table_name}`\n",
    "        \"\"\"\n",
    "    else:\n",
    "        q3a = f\"\"\"create table if not exists `{project_id}`.`nytaxi_stage`.`{table_name}`\n",
    "        ({col_param})\n",
    "        \"\"\"\n",
    "        q3b = f\"\"\"insert into `{project_id}`.`nytaxi_stage`.`{table_name}`\n",
    "        ({col_names})\n",
    "        select *, current_timestamp() from `{project_id}.nytaxi_raw.external_{table_name}`\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "    # get BigQuery connection \n",
    "    client = bigquery.Client(credentials = credentials, project = project_id)\n",
    "\n",
    "    # execute queries \n",
    "    print('creating if not already present schemas')\n",
    "    client.query(q1a)\n",
    "    client.query(q1b)\n",
    "    client.query(q1c)\n",
    "    client.query(q1d)\n",
    "\n",
    "    print('creating external table')\n",
    "    time.sleep(10)\n",
    "    client.query(q2)\n",
    "\n",
    "    print('populating stage table')\n",
    "    time.sleep(10)\n",
    "    client.query(q3a)\n",
    "    time.sleep(10)\n",
    "    client.query(q3b)\n",
    "\n",
    "    print('loading data to stage complete')\n",
    "\n",
    "    print('cleanuped up env for next run')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # print(os.system('pwd'))\n",
    "    # print('parsing input arguments')\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # parser.add_argument('--table_name')\n",
    "    # parser.add_argument('--start_date')\n",
    "    # parser.add_argument('--end_date')\n",
    "    \n",
    "    # args = parser.parse_args()\n",
    "    \n",
    "    # date ranges\n",
    "    start_dt = datetime.strptime('2010-01-01','%Y-%m-%d')#datetime.strptime(args.start_date,'%Y-%m-%d')\n",
    "    end_dt = datetime.strptime('2010-01-01','%Y-%m-%d')\n",
    "    delta = relativedelta(months=1)\n",
    "    \n",
    "    # var that stays the same through out run \n",
    "    table_name = 'yellow_tripdata'#args.table_name\n",
    "    bucket_name_data = 'taxi-data-extract'\n",
    "    gcp_project_name = 'pipeline-analysis-446021'\n",
    "    \n",
    "    print(f\"will be fetching parquest for {table_name}, from {start_dt} - {end_dt}\")\n",
    "\n",
    "#########################################local#############################################################################\n",
    "    os.system('gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS')\n",
    "###########################################################################################################################\n",
    "    \n",
    "    # start spark session \n",
    "    print('starting spark session')\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[4]\") \\\n",
    "        .appName('extract-load-spark') \\\n",
    "        .getOrCreate()\n",
    "        \n",
    "    while start_dt <= end_dt:\n",
    "    \n",
    "        # vars for fetching parquet\n",
    "        year_month = start_dt.strftime(\"%Y-%m\")\n",
    "        filename = f\"{table_name}_{year_month}.parquet\"\n",
    "        url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}\"\n",
    "        \n",
    "        print(f'starting iteration for {table_name}, {year_month}')\n",
    "        \n",
    "        # vars for gcp \n",
    "        root_path = f\"gs://{bucket_name_data}/{datetime.now().strftime('%Y-%m-%d_%H:%M:%S')}_{table_name}_{year_month}\"\n",
    "        \n",
    "        # running throught the ETL pipeline\n",
    "#########################################local#############################################################################\n",
    "        df = load_trip_data(spark, url, filename)\n",
    "#########################################dataproc##########################################################################\n",
    "        # df = load_trip_data(spark, url, filename, bucket_name)\n",
    "###########################################################################################################################\n",
    "        if parquet_not_downloaded(df):\n",
    "            abort_pipeline\n",
    "            pass\n",
    "        elif parquet_downloaded(df):\n",
    "            df = dimension_name_cleanup(df)\n",
    "#########################################local#############################################################################\n",
    "            data_2_gcp_cloud_storage(df, table_name, year_month, root_path, filename)\n",
    "#########################################dataproc##########################################################################\n",
    "            # data_2_gcp_cloud_storage(df, root_path)\n",
    "###########################################################################################################################\n",
    "            bucket_2_bigquery(gcp_project_name, table_name, root_path)\n",
    "        else:\n",
    "            print('another issue encountered not yet considered, halting iterations')\n",
    "            break\n",
    "    \n",
    "        start_dt += delta\n",
    "    \n",
    "    # close spark session\n",
    "    # spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
